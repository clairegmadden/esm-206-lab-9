---
title: "Lab 9 - Multiple Linear Regression"
author: "Claire Madden"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Use this template to follow along in Lab Week 9. Each code chunk you'll need is already created and named. 

**Lab 9 Objectives:**

- Explore multivariate data (SLO housing prices)
- Perform multiple linear regression
- Assess diagnostics
- Compare different models by AIC
- Explain model outputs
- Make a nice table of regression results
- Make predictions using a final model
- Visualize predictions with original data

###1. Load packages

- tidyverse
- stargazer

```{r packages, include = FALSE}

# a. Load packages 'tidyverse' and 'stargazer':
library(tidyverse)
library(stargazer)


```

###2. Load data (slo_homes.csv) as a df called 'homes', then filter to create a data frame called 'homes_sub' that only include homes in SLO, Arroyo Grande, Santa Maria-Orcutt, and Atascadero

```{r get_data, include = FALSE}

# a. Read in data as 'homes':

homes <- read_csv("slo_homes.csv")

# b. Filter to only include cities "San Luis Obispo", "Santa Maria-Orcutt", "Atascadero", and "Arroyo Grande", and call this new subset 'homes_sub':

homes_sub <- homes %>%
  filter(City == "Arroyo Grande" | City == "San Luis Obispo" | City == "Atascadero" | City == "Santa Maria-Orcutt")


```

###3. Go exploring (visual) + think critically about variables

*Note: It's OK to LOOK at things separately, even if you're including all in a model together!*

Example: if we want to compare distribution of housing prices by CITY (ignoring all other variables), we can do that:

```{r by_city}

# a. Calculate mean price by city
mean_by_city <- homes_sub %>% 
  group_by(City) %>% 
  summarize(
    mean = mean(Price)
  )

# b. Visualize prices by city
# density distribution = bins are centered around observations not set intervals
by_city <- ggplot(homes_sub, aes(x = Price)) +
  geom_density(aes(color = City, fill = City), alpha = 0.3) + # Note: just to show what the geom_violin shows
  theme_classic() +
  scale_x_continuous(expand = c(0,0), limits = c(0,3e6)) +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "Home Prices (USD)", y = "Density")

by_city

# noticeable: Santa Maria - bulk of housing prices look lower than other three groups

```

Or another question: Overall relationship between home square footage and price, separated by City? 

```{r by_sqft}

# a. Relationship between square footage and price
by_sqft <- ggplot(homes_sub, aes(x = SqFt, y = Price)) +
  geom_point(aes(color = City, pch = Status), alpha = 0.5) +
  facet_wrap(~Status)

by_sqft

# Observations here: Does relationship appear ~ linear? YES 
# Anything else we can pick out re: trends, outliers, etc.? What is the general trend? Any outliers? Is there reason enough for us to omit it?

# overall makes sense to describe relationship using multiple linear regression - observations are randomly collected
# normality of residuals?
# variance change?
# need to create the model FIRST to test these things!

```

###4. Multiple linear regression

Multiple linear regression in R follows the same syntax we've been using so far: 

    lm(y ~ x1 + x2 + x3..., data = df_name)
    
Let's try this model a couple of different ways: 

(1) Use all available variables (saturated model) 
(2) Use only SqFt as a predictor for "home size" generally (omit Bathrooms and Bedrooms), and omit PricePerSqFt (since it's derived from two other existing variables in the model)

Use summary() to view the model output and statistics.

```{r saturated}

# trying to predict the size of a house as it relates to predictor variables

# a. Saturated model: 

homes_lm1 <- lm(Price ~ City + Bedrooms + Bathrooms + SqFt + PricePerSqFt + Status, data = homes_sub)

# PricePerSqFt makes no sense to include because using a variable calculated from dependent variable Price --- Biased overfitting of model results
# reference level for sale status - Forclosure, for City - Arroyo Grande (the ones that dont show up in the output)
# output suggests the price of regular house is less than price of forclosed house, increase in bedroom is equal to $30,000 reduction in price
# WHATS HAPPENING - if variables exist in a model are very strongly correllated you might get colinarity and a misleading model
# which three predictor variables are all telling kind of the same thing about the house - bedrooms, bathrooms and sqft all are telling us something about how big the house is
# just pick one! 



# b. Model summary:

summary(homes_lm1)
# estimates for coefficients, error, hypothesis test of each coefficient

```

The next model: Exclude price per square foot, and bedrooms/bathrooms (since these are largely explained by square footage...)

```{r subset}

# a. Updated model with variables City, SqFt, and Status:

homes_lm2 <- lm(Price ~ City + SqFt + Status, data = homes_sub)

homes_lm2

# slope of a coefficient is how much the outcome variable will change if all else is the same
# if everything else is the same, we would expect a house in SLO to sell for $34,525 more than the same one in Arroyo Grande
# other things make sense in this model!
# price per sqft is about $250 - just about average!
# coefficient for "status regular" means that all else exactly the same, a house with a regular status will sell for about $210,000 more than a house in forclosure


# b. Model summary:

summary(homes_lm2)

# status of short sale - highly nonsignificant value - not a big difference in price of house sold in short sale vs. forclosure
# status regular - significant
# city that has a housing price most similar to Arroyo Grande = SLO, both other cities prices are significantly different than Arroyo Grande
# use adjusted R - squared for multiple linear regression - NO SUCH THING AS A GOOD OR BAD R SQUARED VALUE
# overall pvalue - the model significantly predicts housing prices better than random chance would

```

Wait...but what if I wanted everything to be with respect to a Regular sale status? Then I need to change my factor levels. We've done this before, here we'll use a different function (fct_relevel) from *forcats* package in the tidyverse. 

```{r fct_relevel}

# a. Set Status to class 'factor'
homes_sub$Status <- factor(homes_sub$Status)


# b. Check to ensure it's a factor now
# use class() in console to check!

# c. Check levels:
levels(homes_sub$Status)
# output tells you the order of the factor levels

# d. Reassign reference level of "Status" to "Regular":
homes_sub$Status <- fct_relevel(homes_sub$Status, "Regular")


# e. Now run the regression again - same equation, but now the reference level is different (Status = Regular): 

homes_lm3 <- lm(Price ~ City + SqFt + Status, data = homes_sub)

summary(homes_lm3)
```

Interpret the coefficients and statistical outcomes above. 
Notes: 

###5. Model diagnostics

Remember, since we're concerned about *residuals* (distance that actual observations exist from model predictions), we can only evaluate some assumptions *after* running the regression. 

Then we can evaluate model diagnostics using the plot() function:

```{r diagnostics}

# a. Model diagnostics:

plot(homes_lm3)

# residual vs fitted - does it look like vertical scatter is getting way bigger? not really anything to worry about
# qq plot - does it look like overall residuals are pretty normally distributed? YES! dont let a few values throw you off
# ignore the red line on these plots!
# overall, heteroscedasticity is OK, residuals normality definitely looks good, conceptually and mathematically my model is making sense!



```

###6. Model comparison by Akaike Information Criterion

The AIC is a quantitative metric for model "optimization" that balances complexity with model fit. The best models are the ones that fit the data as well as possible, as simply as possible. Recall: lower AIC value indicates a *more optimal* balance - **BUT STATISTICS IS NO SUBSTITUTE FOR JUDGEMENT!!!**

```{r AIC}

# a. AIC values for each model: not a way out of thinking critically about model inputs!

sat_aic <- AIC(homes_lm1)
sat_aic
final_aic <- AIC(homes_lm3)
final_aic
# a lower AIC value shows the model that has the best balance of complexity and predictive capability (SHOULD NEVER BASE A MODEL DECISION SOLELY ON THIS)

# Answer: which would you pick? 
# I would still pick homes_lm3 because : heteroscedasticity is OK, residuals normality definitely looks good, conceptually and mathematically my model is making sense!

```

###7. Regression tables with *stargazer*

```{r stargazer, results = 'asis'}

# a. Prepare a nice regression table:
lm_table <- stargazer(homes_lm1, homes_lm3, type = "html")


# to get table into word format - open html doc from github folder as a word doc and then edit table or open as html and then copy and paste into word 

# Note: If you want to work with this in Word, save to html, open, copy and paste into Word. 

```

###8. Making predictions

Using your final selected model, predict the housing price for a range of home sizes, sale status, and city. 

The predict() function uses the following syntax:

      predict(model_name, newdata = new_data_name)
      
Defaults are to exclude the prediction SE and mean confidence interval - if you want to include, use arguments

      se.fit = TRUE
      interval = "confidence" 
      interval = "prediction"

First, you need to create a new data frame of values that contain ALL NECESSARY VARIABLES **with the same variable names AND level strings**.

```{r df_new}

# First, make a new data frame

# Note that the df_new created below has the SAME variable names and level strings as the original model data (otherwise R won't know how to use it...)
# Work through this on your own to figure out what it actually does:

df_new <- data.frame(City = rep(c("San Luis Obispo",
                                  "Santa Maria-Orcutt",
                                  "Atascadero",
                                  "Arroyo Grande"), 
                                each = 60), 
                     SqFt = rep(seq(from = 500,
                                    to = 3500, 
                                    length = 20), 
                                times = 12), 
                     Status = rep(c("Regular",
                                    "Foreclosure",
                                    "Short Sale"), 
                                  times = 12, 
                                  each = 20))


```

Make predictions for the new data using predict():

```{r predict}

# a. Make predictions using the new data frame:

price_predict <- predict(homes_lm3, 
                         newdata = df_new, 
                         se.fit = TRUE, 
                         interval = "confidence")


# b. Bind predictions to the data to make it actually useful:

predict_df <- data.frame(df_new, price_predict)
# data.frame function lumps things together by column as long as they have the same rows


```

Then visualize it!

```{r graph, echo = FALSE}

# Create a line graph with square footage on the x-axis, predicted price on y-axis, with color dependent on City and facet_wrapped by sale status (follow along):

predict_graph <- ggplot(predict_df, aes(x = SqFt, y = fit.fit))+
  geom_line(aes(color = City))+
  facet_wrap(~Status)

predict_graph

#prediction for homes in different cities for different squarefootage sold in different sale statuses

```

END LAB